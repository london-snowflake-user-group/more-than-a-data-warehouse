{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "ImportLibraries",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Standard library imports\nimport time\nfrom datetime import datetime\n\n\n\n# Third party imports\nimport streamlit as st\nimport altair as alt\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings; warnings.simplefilter('ignore') # We sometimes get warnings in cell outputs, stop these from being displayed.\n\nfrom snowflake.snowpark import Session, context, functions\nfrom snowflake.snowpark.types import DateType, FloatType, IntegerType, StringType, StructField, StructType, Variant\nfrom snowflake.ml import dataset, registry\nfrom snowflake.ml.modeling import pipeline, preprocessing, metrics, model_selection, xgboost\nfrom snowflake.core import Root, CreateMode, task\n\n\n\n# Local application imports\n# None\n\n\n\n# Get a Snowflake session, this is where you would normally login, instead just use get_active_session!\nsf_session = get_active_session()\n\n\n\n# Specify which DB, Schema and Tables we'll be using.\ndb_name = 'INSURANCE_2'\nschema_name = 'PUBLIC'\nclient_raw_data_table_name = db_name + '.' + schema_name + '.' + 'CLIENTS'\nclient_features_table_name = db_name + '.' + schema_name + '.' + 'CLIENT_FEATURES'\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "RawDataReview",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Features shown here:\n# ðŸ“˜ Snowflake Notebooks is a development interface offering a cell-based programming environment.\n#    https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks\n\n# ðŸ“˜ Snowpark API provides an intuitive library for querying and processing data at scale in Snowflake.\n#    https://docs.snowflake.com/en/developer-guide/snowpark/index\n\n# ðŸ“˜ Streamlit is a Python library that makes it easy to create custom web apps for machine learning and data science.\n#    https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit\n\n\n\n# ðŸ¦œ What does the data we'll be working with look like? Let's use the Snowpark API to see.\n\n\n\n# To create a DataFrame stored in Snowflake we can simply call the table method.\nclients_df = sf_session.table( client_raw_data_table_name )\n\n\n\n# You can use Streamlit components in Notebooks since they all run on Snowflake.\nst.dataframe( clients_df.first(10) )\n\n\n\n# ðŸ¾ Notebooks don't display tables of data nicely, meaning you often end up importing\n#    other libraries. With Notebooks & Streamlit you no longer need to do that, reducing dependencies, \n#    complexity and ultimately saving you development time.",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8b777c01-dda7-401b-91b1-66c9aeff1974",
   "metadata": {
    "language": "python",
    "name": "SimpleSnowflakeMLDemo",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowpark ML Modeling API for feature engineering and preprocessing\n#    https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling\n#    https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling\n\n\n# ðŸ¦œ The Snowpark ML Modeling API uses familiar Python frameworks such as scikit-learn and XGBoost for \n#    preprocessing data, feature engineering, and training models inside Snowflake. We need to encode \n#    some of the values in our dataset so they can be used by our model later. To do this we'll use \n#    the snowflake.ml.modeling.preprocessing.OrdinalEncoder, which is based on scikit's  encoder \n#    sklearn.preprocessing.OrdinalEncoder, so it should be what you know and love!\n\n\n\n# Let's turn the gender and smoker fields into 1's and 0's by creating an encoder.\nordinal_encoder = preprocessing.OrdinalEncoder(\n    input_cols = ['GENDER', 'SMOKER', 'COUNTY'],\n    output_cols = ['GENDER_ENCODED', 'SMOKER_ENCODED', 'COUNTY_ENCODED'],\n    encoded_missing_value = '999',\n    drop_input_cols = True\n)\n\n\n\n# Now we can apply it to the dataframe which holds our raw client data, and create a new dataframe\n# which has the original columns, which may hold PII data, replaced with encoded values.\nclient_features_df = ordinal_encoder.fit( clients_df ).transform( clients_df )\n\n\n\n# The Gender and Smoker fields are now 0 or 1 instead of text.\nst.dataframe( client_features_df.first(10) )\n\n\n\n# ðŸ¾ These ready-to-use ML Functions, based on common libraries such as sklearn, can help shorten \n#    development time and democratize ML across your organization.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6057aea7-976c-419e-a340-984df0388fb4",
   "metadata": {
    "language": "python",
    "name": "FeatureEngineeringFunctionAndSP",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowpark ML Modeling API for feature engineering and preprocessing\n#    https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling\n#    https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/modeling\n\n# ðŸ“˜ Snowpark API provides an intuitive library for querying and processing data at scale in Snowflake.\n#    https://docs.snowflake.com/en/developer-guide/snowpark/index\n\n\n\n# ðŸ¦œ We want to regularly update the data our models use for training and fitting, and so we'll\n#    automate this by creating a Python function locally which is then saved as a Stored Procedure\n#    in Snowflake so it can be called whenever we, or anyone else, needs it.\n\n\n\n# Let's define the Python function locally first.\ndef encodeColumns( \n    session: Session, \n    raw_table: str, \n    processed_table: str, \n    input_cols: list, \n    output_cols: list, \n    encoded_missing_value: int \n) -> str:\n\n    # Third party imports\n    from snowflake.ml.modeling import preprocessing, pipeline\n    \n    # With scikit-learn, it is common to run a series of transformations using a \"pipeline\". \n    # scikit-learn pipelines do not work with Snowpark ML classes, so Snowpark ML provides a \n    # Snowpark Python version of sklearn.pipeline.Pipeline for running a series of transformations. \n    # This class is in the snowflake.ml.modeling.pipeline package, and it works the same as \n    # the scikit-learn version.\n    preprocessing_pipeline = pipeline.Pipeline(\n        steps=[\n            (\n                'OrdinalEncoding',\n                preprocessing.OrdinalEncoder( \n                    input_cols = input_cols,\n                    output_cols = output_cols,\n                    encoded_missing_value = encoded_missing_value,\n                    drop_input_cols = True\n                )\n            )\n        ]\n    )\n\n    # Where are we getting the data from?\n    df = session.table( raw_table )\n\n    # Apply the pipeline.\n    features_df = preprocessing_pipeline.fit( df ).transform( df )\n\n    # Write the table to Snowflake.\n    features_df.write.mode( 'overwrite' ).save_as_table( processed_table )\n\n    # Provide a message back.\n    print( f'Feature Engineering done, table created: {processed_table}' )\n    \n    return processed_table\n\n\n\n# Create a Stored Procedure which uses the Python function we created above.\nstored_procedure_object = functions.sproc(\n    name = 'ENCODE_COLUMNS',\n    func = encodeColumns,\n    is_permanent = True,\n    stage_location = '@ML_MODELS',\n    replace = True,\n    packages = ['snowflake-snowpark-python', 'snowflake-ml-python'],\n)\n\n\n\n# ðŸ¾ The Snowpark ML Modeling API allows you to build the ML pipelines using the libraries \n#    you know and love without needing to learn anything new.\n\n# ðŸ¾ Python developers can build locally then push their work into Snowflake for others\n#    to use who may be more familiar using SQL. No need to re-write Python into SQL and\n#    vice versa, saving you time, effort and development cost.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3638b320-535c-4757-9d5c-5c1187eab46a",
   "metadata": {
    "language": "python",
    "name": "ScheduleSprocToRun",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowflake Python API provides comprehensive APIs for interacting with Snowflake resources across \n#    data engineering, Snowpark, Snowpark ML, and application workloads using a first-class Python API.\n#    https://docs.snowflake.com/en/developer-guide/snowflake-python-api/reference/latest/index\n\n\n\n# ðŸ¦œ We're going to use a Task to schedule the Stored Procedure to run every day so we have up-to-date\n#    data to use in our models. We'll do this by creating the Task through the Python API.\n\n\n\n# This is the entry point of the Snowflake Core Python APIs that manage the Snowflake objects.\nroot = Root( sf_session )\n\n\n\n# This is where we'll store the Task, its like setting the context in Worksheets.\ntasks_context = root.databases[db_name].schemas[schema_name].tasks\n\n\n\n# Create a Task in Snowflake, scheduled to run at 10am every day.\ntasks_context.create( \n    task.Task(\n        'CALL_ENCODE_COLUMNS',\n        task.StoredProcedureCall( stored_procedure_object ),\n        warehouse = 'COMPUTE_WH',\n        schedule = task.Cron( '10 * * * *', 'Europe/London' ),\n    ),\n    mode = CreateMode.or_replace,\n)\n\n\n\n# What did we create?\nst.dataframe( sf_session.sql( 'show tasks' ) )\n\n\n\n# ðŸ¦œ As we've seen, Tasks can be defined to automate data engineering workflows, specifically \n#    recurring tasks, natively in Snowflake. However, writing a full-scale pipeline with many \n#    tasks has been hard. Often, customers have built their own framework on top of Snowflake \n#    to solve a number of programmability issues such as constructing logical groups of tasks, \n#    retries, dynamic behavior, configurability, logging, etc.\n\n#    To start addressing these needs, Snowflake introduced the following features:\n#    + Richer Task Graph Features: Finalizer Tasks, Runtime Reflection Variables, Task \n#      Predecessor Return Values, etc.\n#    + SNOWFLAKE PYTHON API: Provides Python APIs for Snowflake resources.\n#    + Enhanced Task UI in Snowsight: Enables users to easily manage and monitor all tasks \n#      from a scheduled graph in one place.\n#    + Snowpark ML API: Python API for preprocessing, feature engineering, and training models\n#      following familiar Python frameworks such as scikit-learn and XGBoost.\n#    + Snowpark Model Registry: Snowpark Model Registry allows customers to securely manage \n#      models and their metadata in Snowflake, regardless of origin.\n\n# ðŸ”— https://medium.com/snowflake/productionize-your-ml-workflow-using-snowflake-task-dag-apis-8470aa33172c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a82a960-695b-4329-98aa-3de0a1cd43c2",
   "metadata": {
    "language": "python",
    "name": "CallFeatureEngineeringSproc",
    "collapsed": false
   },
   "outputs": [],
   "source": "# ðŸ¦œ Rather than waiting for a day, let's call the sproc directly to engineer the features now.\n\n\n\n# Use Snowpark to call the Stored Procedure.\nsf_session.call( \n    'ENCODE_COLUMNS', # The sproc name we want to call.\n    client_raw_data_table_name, # The table holding our raw data.\n    client_features_table_name, # The table we'll create which will have our engineered data.\n    ['GENDER', 'SMOKER', 'COUNTY'], # Columns to encode.\n    ['GENDER_ENCODED', 'SMOKER_ENCODED', 'COUNTY_ENCODED'], # Column names added to the raw data with encoded data.\n    999\n)\n\n\n\n# What have we created?\nclient_features_df = sf_session.table( client_features_table_name )\n\n\n\n# You can use Streamlit components in Notebooks since they all run on Snowflake.\nst.dataframe( client_features_df.first(10) )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "892b7b72-2596-4a34-b7d4-4c2536f14383",
   "metadata": {
    "language": "python",
    "name": "CreatingDatasets",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Datasets to hold collections of data organized into versions\n#    https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/dataset\n\n\n\n# ðŸ¦œ Datasets are Snowflake schema-level objects specially designed for machine learning \n#    workflows. Snowflake Datasets hold collections of data organized into versions, where \n#    each version holds a materialized snapshot of your data with guaranteed immutability, \n#    efficient data access, and interoperability with popular deep learning frameworks.\n\n\n\n# Generate a random version number, it must start with letter or digit, and followed by \n# letter, digit, '_', '-' or '.'. The length limit is 128.\nversion_number = 'v' + datetime.now().strftime( '%Y-%m-%d_%H-%M-%S' )\n\n\n\n# Materialize DataFrame contents into a Dataset and version control it.\nclient_features_ds = dataset.Dataset.create(\n    sf_session,\n    name = db_name + '.' + schema_name + '.' + 'CLIENT_FEATURES_DS',\n    exist_ok = True,\n)\n\n\n\nclient_features_dsv = client_features_ds.create_version(\n    version = version_number,\n    input_dataframe = sf_session.table( client_features_table_name ).drop( ['ID', 'FIRST_NAME', 'LAST_NAME'] ),\n)\n\n\n\n# Load into a Snowpark Dataframe to display.\nclient_features_df = client_features_dsv.read.to_snowpark_dataframe()\nst.subheader( 'This is in effect, simply the data from the CLIENT_FEATURES table...' )\nst.dataframe( client_features_df.first(10) )\n\n\n\n# Let's see what Datasets are in the Account now.\nst.subheader( '...but logged as a Dataset so it can now be for example version controlled.' )\nst.dataframe( sf_session.sql( 'show datasets in account' ) )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "773b763a-a47a-44e1-9275-6faa3bfce9d5",
   "metadata": {
    "name": "WhenShouldYouUseDatasets",
    "collapsed": false
   },
   "source": "## When should you use Datasets?\n\nYou should use Snowflake Datasets when:\n* You need to manage and version large datasets for reproducible machine learning model training and testing.\n* You want to leverage Snowflakeâ€™s scalable and secure data storage and processing capabilities.\n* You need fine-grained file-level access and/or data shuffling for distributed training or data streaming.\n* You need to integrate with external machine learning frameworks and tools.\n\nâ—â—â— Because we used the Snowflake ML API to engineer the data we can see the lineage, from the raw table to the feature engineered table, let's go and take a look in the Lineage UI in Snowsight."
  },
  {
   "cell_type": "code",
   "id": "c998f3b9-b8a1-4eee-a9b7-b412fed02584",
   "metadata": {
    "language": "python",
    "name": "DataExploration",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Common Python packages working with Notebooks\n#    https://repo.anaconda.com/pkgs/snowflake/\n#    https://www.anaconda.com/partners/snowflake\n\n\n\n# ðŸ¦œ In order to use your data you need to understand it, to do this included with\n#    you need to visualise it, Snowpark includes a number of popular open source \n#    third-party Python packages that are built and provided by Anaconda are made \n#    available to use out of the box inside Snowflake virtual warehouses.\n\n\n\n# Get the dataframe we created earlier and check it for correlated features using\n# the correlation function in snowflake.ml.modeling.metrics.\ncorr_clients_df = metrics.correlation( \n    df = client_features_dsv.read.to_snowpark_dataframe()\n)\n\n\n\n# Generate a heatmap with the features so we can see which have the most effect.\nmask = np.triu(\n    np.ones_like(\n        corr_clients_df, \n        dtype=bool\n    )\n)\n\n\n\nplt.figure( figsize=( 3, 3 ) )\nplt.rc( 'xtick', labelsize=5 )\nplt.rc( 'ytick', labelsize=5 )\nheatmap = sns.heatmap(\n    corr_clients_df, \n    mask=mask, \n    cmap=\"YlGnBu\", \n    annot=False, \n    vmin=-1, \n    vmax=1\n)\n\n\n\n# ðŸ¾ Access more than 2,000 Python packages directly from within Notebooks, \n#    Streamlit, Stored Procedures, Python Worksheets, etc.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59a07d34-9c7d-4d6d-aa63-094f924b7e68",
   "metadata": {
    "language": "python",
    "name": "InteractiveDataExploration",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Streamlit\n#    https://docs.snowflake.com/en/developer-guide/streamlit/about-streamlit\n\n\n\n# ðŸ˜• However, I've always found matplotlib's charts a little fixed, not very \n#    interactive and to be honest not very attractive. What could we use \n#    instead? Streamlit of course! Streamlit is great as it allows you to \n#    build interactive UI's quickly and easily.\n\n\n\n# Get our client features data from the Dataset and summarise it.\nclient_features_summed_df = client_features_dsv.read.to_snowpark_dataframe().to_pandas().groupby(\n    ['SMOKER_ENCODED', 'AGE', 'CHILDREN']\n).aggregate(\n    {'TOTAL_CLAIMS':['min', 'max', 'mean', 'count']}\n).reset_index()\n\n\n\n# After summarising, munge it a bit to get rid of multi-level data structures, etc.\nclient_features_summed_df.columns = client_features_summed_df.columns.map( '_'.join )\n\n\n\n# Lets create some UI components that will allow users to choose their own measures\n# and groupings to plot on a chart.\n\n# ðŸ¾ This means you as a developer can build self-service into your work, reducing\n#    your workload and making you, and your customers, more productive.\n\ncol1, col2 = st.columns(2)\n\nwith col1:\n    bar_colour_scheme = st.selectbox( \n        label = 'Colour Scheme',\n        options = ('category20', 'category20', 'tableau10', 'tableau20', 'teals', 'plasma', 'rainbow')\n    )\n    \n    measure_to_display_dict = {\n        'Min': 'TOTAL_CLAIMS_min', \n        'Max': 'TOTAL_CLAIMS_max', \n        'Mean': 'TOTAL_CLAIMS_mean', \n        'Count': 'TOTAL_CLAIMS_count'\n    }\n    \n    measure_to_display = st.radio(\n        label = 'Choose a measure to plot', \n        options = ['Min', 'Max', 'Mean', 'Count'], \n        horizontal = True, \n    )\n\nwith col2:\n    color_to_display_dict = {\n        'Smoker': 'SMOKER_ENCODED_', \n        'Children': 'CHILDREN_'\n    }\n    \n    color_to_display = st.selectbox(\n        label = 'Choose a grouping to plot', \n        options = ['Smoker', 'Children'], \n    )\n\n\n\n# Now we can build a chart object.\nchart = alt.Chart(client_features_summed_df).mark_circle(size=60).encode(\n    x=alt.X(\n        'AGE_:O', \n        axis=alt.Axis( title='Age', grid=True, format=',.0f' )\n    ),\n    y=alt.Y(\n        f'{measure_to_display_dict[measure_to_display]}:Q', \n        axis=alt.Axis( title=f'{measure_to_display}', grid=True, format=',.0f' )\n    ),\n    color=alt.Color(\n        f'{color_to_display_dict[color_to_display]}:O', \n        legend=alt.Legend( orient=\"bottom\", columns=4, title=f'{color_to_display}' ),\n        scale=alt.Scale( scheme=bar_colour_scheme ),\n    ),\n    tooltip = [\n        alt.Tooltip( 'AGE_', title='Age' ),\n        alt.Tooltip( 'SMOKER_ENCODED_', title='Smoker?' ),\n        alt.Tooltip( 'TOTAL_CLAIMS_min', title='Min', format=',.0f' ),\n        alt.Tooltip( 'TOTAL_CLAIMS_max', title='Max', format=',.0f' ),\n        alt.Tooltip( 'TOTAL_CLAIMS_mean', title='Mean', format=',.0f' ),\n        alt.Tooltip( 'TOTAL_CLAIMS_count', title='Count' ),\n    ]\n).interactive()\n\n\n\n# Then display it.\nst.altair_chart( chart, use_container_width=True )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f7464a6-80f3-43e3-a5ce-241bef5580e0",
   "metadata": {
    "language": "python",
    "name": "CreateAndTrainAModel",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowpark ML Modeling API for model development and training\n#    https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling\n\n\n\n# ðŸ¦œ Now our data is sorted, lets build and train a machine learning model that will be\n#    used to predict the amount of claims a customer may make.\n\n\n\n# Split the data into train and test sets\ntrain_df, test_df = client_features_dsv.read.to_snowpark_dataframe().random_split( weights=[0.9, 0.1], seed=0 )\n\n\n\n# Define the XGBoost model, including distributed Hyperparameter Tuning.\ntuned_xgboost_model = model_selection.GridSearchCV(\n    \n    # Choose an algorithm to use by the model.\n    estimator = xgboost.XGBRegressor(),\n\n    param_grid = {\n        # This is where the Hyperparameter Tuning occurs. The following will effectively give us\n        # four differently trained model variations, and use the one with the lowest MAPE, see \n        # scoring below.\n        'n_estimators':[100, 200],\n        'learning_rate':[0.1, 0.2],\n    },\n\n    # Use all cores available on the Warehouse.\n    n_jobs = -1,\n\n    # This is what will be used by the model when called to decide which parameter configuration to use.\n    scoring = 'neg_mean_absolute_percentage_error',\n\n    # Define inputs and outputs.\n    input_cols = ['AGE', 'GENDER_ENCODED', 'CHILDREN', 'BMI', 'SMOKER_ENCODED'],\n    label_cols = ['TOTAL_CLAIMS'],\n    output_cols = ['TOTAL_CLAIMS_PREDICTION'],\n    \n)\n\n\n\n# Fit / train the pipeline to the training data\nfitted_xgboost_model = tuned_xgboost_model.fit( train_df )\n\n\n\n# ðŸ¾ Again, we're using common ML libraries such as sklearn which you already know how to use, \n#    saving you the time and effort of learning something new.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7a60460-3108-465d-83a4-943495fcd7ec",
   "metadata": {
    "language": "python",
    "name": "TestingTheModelDirectly",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowpark ML Modeling API for model development and training\n\n\n\n# ðŸ¦œ Before we make it available for everyone to use lets test it.\n\n\n\n# Test it locally using the test data we created above.\npredicted_claims = fitted_xgboost_model.predict( test_df )\nst.dataframe( predicted_claims.to_pandas().astype(\"float64\") )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5a94c77-d902-4f45-a203-0d585ff1aef3",
   "metadata": {
    "language": "python",
    "name": "LogTheModelInTheRegistry",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowflake Model Registry to securely manage models and their metadata in Snowflake, regardless of origin.\n#    https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview\n\n\n\n# ðŸ¦œ Great, it works, now lets log it into the Snowflake Model Registry so anyone can use it.\n#    The Snowflake Model Registry lets you securely manage models and their metadata in Snowflake, \n#    regardless of origin. Models can have multiple versions, and you can designate a version as the default.\n\n\n\n# Opening the registry so we can use it to add new models and obtain references to existing models.\nmodel_registry = registry.Registry( sf_session )\n\n\n\n# Create a version of the model, if you leave out the version name we create one for you.\nfitted_xgboost_model_version = model_registry.log_model(\n    model = fitted_xgboost_model,\n    model_name = db_name + '.' + schema_name + '.' + 'PREDICT_CLAIMS',\n    # version_name = 'v1', # You don't need to specify a version as one will be automatically generated.\n    comment = 'Model used to predict life insurance claims.',\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cbdcf9a-b86b-4e7b-99ec-304a8c60f198",
   "metadata": {
    "language": "python",
    "name": "WhatDidWeLog",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Lets see what we logged. Beacuse models are logged in Snowflake, you can see them from common\n# views like the INFORMATION_SCHEMA, as well as show commands.\nst.subheader( 'Models Logged' )\nlogged_models = sf_session.sql( 'show models in account;' )\nst.dataframe( logged_models )\n\n\n\nst.subheader( 'Models Versions Logged in PREDICT_CLAIMS' )\npredict_claims_model_versions = sf_session.sql( 'show versions in model ' + db_name + '.' + schema_name + '.' + 'PREDICT_CLAIMS;' )\nst.dataframe( predict_claims_model_versions )\n\n\n\n# ðŸ¾ The model is now logged in a central, secure location which protects both your \n#    data and your intellectual property in one place.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76af2961-1a29-4693-b391-41dbd258937d",
   "metadata": {
    "language": "python",
    "name": "TestingTheModelViaRegistry",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Features shown here:\n# ðŸ“˜ Snowflake Model Registry to securely manage models and their metadata in Snowflake, regardless of origin.\n#    https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview\n\n\n\n# Now lets test the model by calling it via the Model Registry.\nxgboost_model_prediction = fitted_xgboost_model_version.run( test_df, function_name='predict' )\n\nst.dataframe( xgboost_model_prediction.select(['AGE', 'SMOKER_ENCODED', 'TOTAL_CLAIMS', 'TOTAL_CLAIMS_PREDICTION']) )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb19391e-badb-4914-800a-5a1efcb48697",
   "metadata": {
    "name": "HowToUse",
    "collapsed": false
   },
   "source": "So what now? We have a model but one of the questions asked was \"What are the options to surface the output to stakeholders?\"\n\nThere are a few ways we can do that, you can use it in other parts of Snowflake like Worksheets or in tools like Streamlit, lets take a look at how you do that now."
  },
  {
   "cell_type": "code",
   "id": "c79d4cc1-8580-44a7-ba29-b8f73a7ec251",
   "metadata": {
    "language": "python",
    "name": "Resources",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# USEFUL RESOURCES\n# ================\n\n# Medium:\n# https://medium.com/snowflake/getting-started-with-snowpark-model-registry-131e5a2783c4\n# https://medium.com/@soonmo.seong/how-to-use-snowpark-ml-model-registry-in-snowflake-09f93312241b\n# https://medium.com/snowflake/productionize-your-ml-workflow-using-snowflake-task-dag-apis-8470aa33172c\n# https://medium.com/snowflake-engineering/snowflake-python-api-the-key-to-modern-data-pipeline-creation-2ab864c7aa5c\n\n# Documentation:\n# https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview\n\n# Snowflake Labs:\n# Tyler's > https://github.com/Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python/blob/main/Snowpark_For_Python_ML.ipynb\n# https://github.com/Snowflake-Labs/sf-samples/blob/main/samples/ml/mlops_using_model.ipynb\n# https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python/blob/main/1_sf_nb_snowflake_ml_data_ingest.ipynb\n# https://github.com/Snowflake-Labs/sfguide-intro-to-machine-learning-with-snowflake-ml-for-python/blob/main/2_sf_nb_snowflake_ml_feature_transformations.ipynb\n# https://github.com/Snowflake-Labs/sfguide-deploying-custom-models-snowflake-model-registry/blob/main/2_create_and_deploy_custom_model.ipynb\n\n# Quickstarts\n# https://quickstarts.snowflake.com/guide/deploying_custom_models_to_snowflake_model_registry/index.html?index=..%2F..index#0\n\n# Internal\n# https://github.com/michaelgorkow/snowflake_simple_ml/blob/main/demo_notebook.ipynb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a7d0e2a-d21e-4f93-90e9-e5dc91c4c98d",
   "metadata": {
    "language": "sql",
    "name": "CodeHelpers",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- What models have been created?\n-- show models in account;\n-- show versions in model PREDICT_CLAIMS;\n\n\n\n-- Drop a model version, you can do this in Python as well but we'll use SQL here.\n-- alter model PREDICT_CLAIMS drop version POPULAR_CATFISH_2;\n\n\n\n-- Drop a Dataset.\n-- show datasets in account;\n-- drop dataset INSURANCE.PUBLIC.CLIENT_FEATURES_DS;",
   "execution_count": null
  }
 ]
}